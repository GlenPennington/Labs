<!DOCTYPE html>
<html lang="{{ site.lang | default: "en-US" }}">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<link rel="stylesheet" href="minimal.css">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
      <div class="wrapper">
	      <header>
		      <h2><a href="index.html">Psychology Labs and Resources</a></h2>
		      <p><img src="Hope.png" alt="Logo"></p>
		      <h3><a href="https://www.hope.ac.uk/psychology/">Department of Psychology</a></h3>
    
         <p>
		<ul>
			<li><a href="labs.html">Lab Information</a></li>
			<li><a href="form.html">Lab Access Form</a></li>
			<li> <a href="book.html">Book a Lab</a></li>
			<li><a href="equipment.html">Psychology Equipment</a></li>
			<li><a href="psychometrics.html">Psychometric Tests and Tools</a></li>
			<li> <a href="experimentbank.html">PsychoPy Experiment Bank</a></li>
			<li> <a href="stimulusbanks.html">Stimulus Banks</a></li>
			<li> <a href="qualtrics.html">Qualtrics</a></li>
			<li> <a href="psychopy.html">PsychoPy Builder</a></li>
			<li> <a href="stats.html">Lab Manuals &amp; Software</a></li>
			<li> <a href="sona.html">Participant Scheme (SONA)</a></li>
		</ul>
       
      </header>
      
      <section>
        <h1>Stimulus Banks</h1>
	      <table>
		      <tr>
			      <th>Name</th>
			      <th>Reference</th>
			      <th>Link</th>
			      <th>Attribution</th>
			      <th>Type</th>
			      <th>Details</th>
			      <th>Norms/Validity</th>
			      <th>Contact</th>
		      </tr>
<tr><th>100000 humans that don't exist </th><td></td><td>https://generated.photos/humans</td><td>None</td><td>Body</td><td>AI generated human whole body images</td><td>No</td><td></td>
<tr><th>AI generated faces</th><td></td><td>https://generated.photos/faces</td><td>None</td><td>Face</td><td>AI generated human faces</td><td>No</td><td></td>
<tr><th>Glasgow Unfamiliar Face Database</th><td></td><td>http://www.facevar.com/glasgow-unfamiliar-face-database</td><td>None</td><td>Face</td><td>Database of unfamiliar faces</td><td>Yes</td><td>Mike Burton, mike.burton@york.ac.uk</td>
<tr><th>10k US Adult Faces</th><td>Bainbridge, W.A., Isola, P., & Oliva, A. (2013). The intrinsic memorability of face images. Journal of Experimental Psychology: General. Journal of Experimental Psychology: General, 142(4), 1323-1334</td><td>http://wilmabainbridge.com/facememorability2.html</td><td>None</td><td>Face</td><td>10,168 natural face photographs and several measures for 2,222 of the faces, including memorability scores, computer vision and psychology attributes, and landmark point annotations.</td><td></td><td></td>
<tr><th>American Multiracial Face Database</th><td>Chen, J. M., Norman, J. B., & Nam, Y. (2021). Broadening the stimulus set: introducing the American multiracial faces database. Behavior Research Methods, 53(1), 371-389</td><td>https://osf.io/qsdrp/</td><td>CC BY-NC 4.0</td><td>Face</td><td>110 faces, smiling/neutral</td><td>Yes</td><td>https://jacquelinemchen.wixsite.com/sciplab/face-database</td>
<tr><th>Amsterdam Dynamic Facial Expression Set</th><td>Van der Schalk, J., Hawk, S. T., Fischer, A. H., & Doosje, B. J. (in press).Moving faces, looking places: The Amsterdam Dynamic Facial Expressions Set (ADFES), Emotion</td><td>https://aice.uva.nl/research-tools/adfes-stimulus-set/adfes-stimulus-set.html?cb</td><td>None</td><td>Face</td><td>648 dynamic filmed emotional expressions, enjoyment, surprise, fear, sadness, anger, disgust, neutral, contempt, pride, embarrassmenr</td><td>Yes</td><td>https://aice.uva.nl/research-tools/adfes-stimulus-set/request-for-use/request-for-use.html</td>
<tr><th>Warsaw Set of Emotional Facial Expressions</th><td>Olszanowski, M., Pochwatko, G., Kukliński, K., Ścibor-Rylski, M., Lewicki, P., Ohme, R. (2015). Warsaw set of emotional facial expression pictures: a validation study of facial display photographs. Frontiers in Psychology, 5:1516. doi: 10.3389/fpsyg.2014.01516</td><td>http://www.emotional-face.org/wsefep</td><td>None</td><td>Face</td><td>210 photos emotions enjoyment, surprise, fear, sadness, anger, disgust, neutral</td><td></td><td>http://wzzeet.website.pl/wordpress/?page_id=61 </td>
<tr><th>At&T Database of Faces</th><td>Samaria, F. S. (1994). Face recognition using hidden Markov models (Doctoral dissertation, University of Cambridge).</td><td>https://git-disl.github.io/GTDLBench/datasets/att_face_dataset/</td><td>None</td><td>Face</td><td>400 face images, greyscale</td><td></td><td></td>
<tr><th>Basel Face Database</th><td>Walker, M., Schönborn, S., Greifeneder, R., & Vetter, T. (2018). The Basel Face Database: A validated set of photographs reflecting systematic differences in Big Two and Big Five personality dimensions. PloS one, 13(3). doi: https://doi.org/10.1371/journal.pone.0193190</td><td>https://bfd.unibas.ch/en/</td><td>None</td><td>Face</td><td>600 images representing personality dimensions from the Big Two and Big Five</td><td>Yes</td><td></td>
<tr><th>Bogazici Face Database</th><td>Saribay SA, Biten AF, Meral EO, Aldan P, Třebický V, Kleisner K (2018) The Bogazici face database: Standardized photographs of Turkish faces with supporting materials. PLoS ONE 13(2): e0192018. https://doi.org/10.1371/journal.pone.0192018</td><td>https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0192018</td><td>CC BY-NC 4.0</td><td>Face</td><td>Turkish undergraduate faces</td><td>Yes</td><td></td>
<tr><th>Chicago Face Database</th><td>Ma, Correll, & Wittenbrink (2015). The Chicago Face Database: A Free Stimulus Set of Faces and Norming Data. Behavior Research Methods, 47, 1122-1135. https://doi.org/10.3758/s13428-014-0532-5</td><td>https://www.chicagofaces.org/</td><td>None</td><td>Face</td><td>Faces of people between 17-65</td><td>Yes</td><td>bernd.wittenbrink@chicagobooth.edu</td>
<tr><th>Chicago Face Database - Multiracial</th><td>Ma, Kantner, & Wittenbrink, (2020). Chicago Face Database: Multiracial Expansion. Behavior Research Methods. https://doi.org/10.3758/s13428-020-01482-5.</td><td>https://www.chicagofaces.org/</td><td>None</td><td>Face</td><td>Extension of the CFD</td><td>Yes</td><td>bernd.wittenbrink@chicagobooth.edu</td>
<tr><th>Chicago Database- India</th><td>Lakshmi, Wittenbrink, Correll, & Ma (2020). The India Face Set: International and Cultural Boundaries Impact Face Impressions and Perceptions of Category Membership. Frontiers in Psychology, 12, 161. https://doi.org/10.3389/fpsyg.2021.62767</td><td>https://www.chicagofaces.org/</td><td>None</td><td>Face</td><td>Extension of the CFD</td><td>Yes</td><td>bernd.wittenbrink@chicagobooth.edu</td>
<tr><th>Child Affective Facial Expression Set</th><td>LoBue, V. & Thrasher, C. (2015). The Child Affective Facial Expression (CAFE) Set: Validity and reliability from untrained adults. Frontiers in Emotion Science, 5. PDF</td><td>https://www.childstudycenter-rutgers.com/the-child-affective-facial-expression-se</td><td>None</td><td>Face</td><td>1200 images  aged 2-8Childrens facial expressions</td><td>Yes</td><td></td>
<tr><th>Children Spontaneous Facial Expression Video Database</th><td>A novel database of Children's Spontaneous Facial Expressions (LIRIS-CSE). Rizwan Ahmed Khan, Crenn Arthur, Alexandre Meyer, Saida Bouakaz. Image and Vision Computing, Volumes 83–84, March–April 2019. arXiv (2018) preprint, arXiv:1812.01555.</td><td>https://childrenfacialexpression.projet.liris.cnrs.fr/site/requestnew</td><td>None</td><td>Face</td><td>Dynamic clips with prototypic emotions</td><td></td><td></td>
<tr><th>City Infant Faces Database</th><td>Webb, R., Ayers, S. & Endress, A. The City Infant Faces Database: A validated set of infant facial expressions. Behav Res 50, 151–159 (2018). https://doi.org/10.3758/s13428-017-0859-9</td><td>https://docs.google.com/document/d/1_22mp0DXlw-lE9lUplKp_-LrTnD3w_xugfmHAEmaiec/edit</td><td></td><td></td><td>154 infant faces positive/negative/neutral</td><td>Yes</td><td></td>
<tr><th>CMU Multi-PIE Face Database</th><td>Sim, T., Baker, S., & Bsat, M. (2001). The CMU pose, illumination and expression database of human faces. Carnegie Mellon University Technical Report CMU-RI-TR-OI-02</td><td>http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html</td><td>None</td><td>Face</td><td>75000 images of 337 people </td><td></td><td></td>
<tr><th>Complex Emotion Expression Database (CEED)</th><td>Benda MS, Scherf KS (2020) The Complex Emotion Expression Database: A validated stimulus set of trained actors. PLoS ONE 15(2): e0228248. https://doi.org/10.1371/journal.pone.0228248</td><td>https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0228248</td><td>CC BY-NC 4.0</td><td>Face</td><td>Expressions (angry, disgusted, fearful, happy, sad, and surprised) and nine complex expressions (affectionate, attracted, betrayed, brokenhearted, contemptuous, desirous, flirtatious, jealous, and lovesick)</td><td>Yes</td><td></td>
<tr><th>Computer Vision Laboratory Face Database</th><td>Mirage 2003, Conference on Computer Vision / Computer Graphics Collaboration for Model-based Imaging, Rendering, image Analysis and Graphical special Effects, March 10-11 2003, INRIA Rocquencourt, France, Wilfried Philips, Rocquencourt, INRIA, 2003, pp. 38-47.</td><td>http://www.lrv.fri.uni-lj.si/facedb.html</td><td>None</td><td>Face</td><td>Front and side views of faces</td><td></td><td></td>
<tr><th>Dartmouth Database of Children's Faces</th><td>Dalrymple, K. A., Gomez, J., & Duchaine, B. (2013). The Dartmouth Database of Children’s Faces: Acquisition and validation of a new face stimulus set. PloS one, 8(11), e79131</td><td>https://lab.faceblind.org/k_dalrymple/ddcf</td><td>None</td><td>Face</td><td>Children's faces with hair and clothing obscured</td><td>Yes</td><td>kad@umn.edu</td>
<tr><th>Face Database</th><td>Minear, M., & Park, D. C. (2004). A lifespan database of adult facial stimuli. Behavior research methods, instruments, & computers : a journal of the Psychonomic Society, Inc, 36(4), 630–633. https://doi.org/10.3758/bf03206543</td><td>https://agingmind.utdallas.edu/download-stimuli/face-database/</td><td>None</td><td>Face</td><td></td><td></td><td>parklab@utdallas.edu</td>
<tr><th>ChatLab Facial Anomaly Database</th><td>Workman, C. I. & Chatterjee, A. (2021). The Face Image Meta-Database (fIMDb) & ChatLab Facial Anomaly Database (CFAD): Tools for research on face perception and social stigma. Methods in Psychology, 5(100063):1-9.</td><td>https://clffwrkmn.net/cfad/</td><td>None</td><td>Face</td><td>Faces with anomalies</td><td></td><td></td>
<tr><th>Face Recognition Technology</th><td>Phillips, P. J., Martin, A., Wilson, C. L., & Przybocki, M. (2000). An introduction evaluating biometric systems. Computer, 33(2), 56-63</td><td>https://www.nist.gov/programs-projects/face-recognition-technology-feret</td><td>None</td><td>Face</td><td>14,126 images that includes 1199 individuals</td><td></td><td>P. Jonathon Phillips, jonathon.phillips@nist.gov</td>
<tr><th>Face Research Lab - London Set</th><td>DeBruine, Lisa; Jones, Benedict (2017): Face Research Lab London Set. figshare. Dataset. https://doi.org/10.6084/m9.figshare.5047666.v5   </td><td>https://figshare.com/articles/dataset/Face_Research_Lab_London_Set/5047666</td><td>CC BY-NC 4.0</td><td>Face</td><td>102 adult faces 1350x1350 pixels in full color</td><td>Yes</td><td></td>
<tr><th>Faces</th><td>Ebner, N., Riediger, M., & Lindenberger, U. (2010). FACES—A database of facial expressions in young, middle-aged, and older women and men: Development and validation. Behavior research Methods, 42, 351-362. https://doi.org/10.3758/BRM.42.1.351.</td><td>https://faces.mpdl.mpg.de/imeji/</td><td>None</td><td>Face</td><td>172 individuals with six facial expressions</td><td>Yes</td><td></td>
<tr><th>Dynamic Faces</th><td>Holland, C. A. C., Ebner, N. C., Lin, T., & Samanez-Larkin, G. R. (2019). Emotion identification across adulthood using the Dynamic FACES database of emotional expressions in younger, middle aged, and older adults. Cognition and Emotion, 33, 245-257. https://doi.org/10.1080/02699931.2018.1445981.</td><td>https://faces.mpdl.mpg.de/imeji/</td><td>None</td><td>Face</td><td>1026 morphed videos six facial expressions</td><td></td><td></td>
<tr><th>Faces and Motion Exeter Database</th><td>Longmore, C. A., & Tree, J. J. (2013). Motion as a cue to face recognition: Evidence from congenital prosopagnosia. Neuropsychologia, 51, 864-875</td><td>https://chrislongmore.co.uk/famed/</td><td>None</td><td>Face</td><td>32 male actors, filmed from two viewpoints</td><td></td><td>Chris Longmore, chris.longmore@plymouth.ac.uk</td>
<tr><th>FEI Face Database</th><td></td><td>https://fei.edu.br/~cet/facedatabase.html</td><td>None</td><td>Face</td><td>Brazilian face database 200 individuals 2800 images</td><td></td><td>Carlos Eduardo Thomaz, cet@fei.edu.br</td>
<tr><th>FG-NET Database with Facial Expressions and Emotions</th><td>Frank Wallhoff; Bjorn Schuller; Michael Hawellek; Gerhard Rigoll: Efficient Recognition of Authentic Dynamic Facial Expressions on the Feedtum Database IEEE ICME, page 493-496. IEEE Computer Society, (2006)</td><td>https://www.jade-hs.de/team/frank-wallhoff/databases/</td><td>None</td><td>Face</td><td>Six basic emotions</td><td></td><td></td>
<tr><th>Japanese and Caucasian Facial Expressions of Emotion</th><td></td><td>https://www.humintell.com/research-tools/</td><td>Copyright</td><td>Face</td><td>Japanase and caucasion faces with 7 basic expressions</td><td></td><td></td>
<tr><th>Japanese Female Facial Expression</th><td>Lyons, Michael, Kamachi, Miyuki, & Gyoba, Jiro. (1998). The Japanese Female Facial Expression (JAFFE) Dataset [Data set]. Zenodo. https://doi.org/10.5281/zenodo.3451524</td><td>https://zenodo.org/record/3451524#.Y9lKHnbP0dV</td><td>None</td><td>Face</td><td>10 Japanese females, 7 basic expressions</td><td>Yes</td><td></td>
<tr><th>Karolinska Directed Emotional Faces</th><td>Lundqvist, D., Flykt, A., & Öhman, A. (1998). The Karolinska Directed Emotional Faces – KDEF, CD ROM from Department of Clinical Neuroscience, Psychology section, Karolinska Institutet, ISBN 91-630-7164-9.</td><td>https://www.emotionlab.se/kdef/</td><td>None</td><td>Face</td><td>4900 pictures of human facial expressions of emotion</td><td></td><td></td>
<tr><th>Libor Spacek's Facial Images Databases</th><td>D. Hond, L. Spacek `Distinctive Descriptions for Face Processing', Proceedings of the 8th British Machine Vision Conference BMVC97, Colchester, England, pp. 320-329, September 1997</td><td>https://cmp.felk.cvut.cz/~spacelib/faces/</td><td>Link to web page</td><td>Face</td><td>395 individuals, 20 images per model, mostly ages 18-20</td><td></td><td></td>
<tr><th>Make up Datasets</th><td>A. Dantcheva, C. Chen, A. Ross, "Can Facial Cosmetics Affect the Matching Accuracy of Face Recognition Systems?," Proc. of 5th IEEE International Conference on Biometrics: Theory, Applications and Systems (BTAS), (Washington DC, USA), September 2012</td><td>http://www.antitza.com/makeup-datasets.html</td><td>None</td><td>Face</td><td>Faces with and without make up</td><td></td><td></td>
<tr><th>Messiner African American and Caucasian Male Sets</th><td>Meissner, C. A., Brigham, J. C., & Butz, D. A. (2005). Memory for own‐and other‐race faces: A dual‐process approach. Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition, 19(5), 545-567.</td><td>http://iilab.utep.edu/stimuli.htm</td><td>None</td><td>Face</td><td>Male, African American and Caucasian smiling/not smiling</td><td></td><td>Christian Meissener cmeissner@utep.edu</td>
<tr><th>Montreal Set of Facial Displays of Emotion</th><td>Varies</td><td>http://www.psychophysiolab.com/en/msfde.php</td><td>None</td><td>Face</td><td>Expressions of happiness, sadness, anger, fear, disgust, and embarrassment as well as a neutral expression</td><td></td><td>Social Psychophysiology Laboratory, Université du Québec à Montréal</td>
<tr><th>MMI Facial Expression Database</th><td>Valstar, M., & Pantic, M. (2010, May). Induced disgust, happiness and surprise: an addition to the mmi facial expression database. In Proc. 3rd Intern. Workshop on EMOTION (satellite of LREC): Corpora for Research on Emotion and Affect (p. 65).</td><td>https://mmifacedb.eu/</td><td>None</td><td>Face</td><td>Videos and stills prototypical expressions</td><td>Yes</td><td>Dr Yiming Lin</td>
<tr><th>MR2 Face Database</th><td>Strohminger, N., Gray, K., Chituc, V., Heffner, J., Schein, C., and Heagins, T.B. (in press). The MR2: A multi-racial mega-resolution database of facial stimuli. Behavior Research Methods</td><td>https://osf.io/skbq2/</td><td>CC BY-NC 4.0</td><td>Face</td><td>74 multi-racial, mega-resolution database of facial stimuli</td><td>Yes</td><td></td>
<tr><th>NimStim Set of Facial Expressions</th><td>Tottenham, N., Tanaka, J. W., Leon, A. C., McCarry, T., Nurse, M., Hare, T. A., ... & Nelson, C. (2009). The NimStim set of facial expressions: judgments from untrained research participants. Psychiatry Research, 168(3), 242-249.</td><td>https://osf.io/y86rw/wiki/home/</td><td>CC BY-NC 4.0</td><td>Face</td><td>672 images of naturally posed photographs by 43 professional actors (18 female, 25 male) </td><td></td><td></td>
<tr><th>Psychological Image Collection at Sterling</th><td></td><td>http://pics.psych.stir.ac.uk/ESRC/index.htm</td><td>None</td><td>Face</td><td>45 male/54 female</td><td></td><td>Peter Hancock, pjbh1@stir.ac.uk</td>
<tr><th>Radboud Faces Database</th><td>Langner, O., Dotsch, R., Bijlstra, G., Wigboldus, D.H.J., Hawk, S.T., & van Knippenberg, A. (2010). Presentation and validation of the Radboud Faces Database. Cognition & Emotion, 24(8), 1377—1388. DOI: 10.1080/02699930903485076</td><td>https://rafd.socsci.ru.nl/RaFD2/RaFD?p=main</td><td>None</td><td>Face</td><td>67 models, caucasian, adults and children 8 emotional expressions</td><td></td><td>info@rafd.nl</td>
<tr><th>RADIATE</th><td>Conley, M. I., Dellarco, D. V., Rubien-Thomas, E., Cohen, A. O., Cervera, A., Tottenham, N., & Casey, B. J. (2018). The racially diverse affective expression (RADIATE) face stimulus set. Psychiatry research.</td><td>http://fablab.yale.edu/page/assays-tools</td><td>None</td><td>Face</td><td>face stimulus set of 1721 racially diverse expressions</td><td></td><td></td>
<tr><th>Sheffield Face Database</th><td>Wechsler, H., Phillips, J. P., Bruce, V., Soulie, F. F., & Huang, T. S. (Eds.). (2012). Face recognition: From theory to applications (Vol. 163). Springer Science & Business Media.</td><td>https://www.visioneng.org.uk/datasets/</td><td>None</td><td>Face</td><td>564 images of 20 individuals (mixed race/gender/appearance)</td><td></td><td></td>
<tr><th>Todorov Synthetic Faces Databases</th><td>Varies</td><td>https://tlab.uchicago.edu/databases/</td><td>None</td><td>Face</td><td>Several databases of computer generated synthetic faces</td><td></td><td>https://www.chicagobooth.edu/faculty/directory/t/alexander-todorov</td>
<tr><th>UB KinFace</th><td> Siyu Xia, Ming Shao, Jiebo Luo, and Yun Fu, “Understanding Kin Relationships in a Photo”, IEEE Transactions on Multimedia (T-MM), Volume: 14, Issue: 4, Page(s): 1046-1056, 201</td><td>http://www1.ece.neu.edu/~yunfu/research/Kinface/Kinface.htm</td><td>None</td><td>Face</td><td>600 images of 400 people which can be separated into 200 groups. Each group is composed of child, young parent and old parent images</td><td></td><td></td>
<tr><th>Yale Face Database</th><td>Varies</td><td>http://cvc.cs.yale.edu/cvc/projects/yalefaces/yalefaces.html#:~:text=The%20Yale%20Face%20Database%20(size,sleepy%2C%20surprised%2C%20and%20wink.</td><td>None</td><td>Face</td><td>165 grayscale images in GIF format of 15 individuals</td><td></td><td></td>
<tr><th>IUAV image dataset</th><td>Bertamini, M. & Sinico, M. (2019). A study of objects with smooth or sharp features created as line drawings by individuals trained in design. Empirical Studies of the Arts, doi: 10.1177/0276237419897048 </td><td>https://osf.io/cx62j/</td><td>None</td><td>Objects</td><td>772 images  line drawings object classification, category (computer vs free hand drawing)</td><td></td><td></td>
<tr><th>Action Database</th><td>Keefe, B.D., Villing, M., Racey, C., Strong, S.M., Wincenciak, J., Barraclough, N.E. (2014) A database of whole-body action videos for the study of action, emotion and untrustworthiness trustworthiness discrimination. Behavior Research Methods doi:10.3758/s13428-013-0439-</td><td>https://www-users.york.ac.uk/~neb506/databases.html</td><td>None</td><td>Movement</td><td>whole-body action videos for the study of action, emotion, untrustworthiness, identity, and gender.</td><td></td><td></td>
<tr><th>Bodily Expressive Action Stimulus Test</th><td>de Gelder, B. & Van den Stock, J. (2011). The Bodily Expressive Action Stimulus Test (BEAST). Construction and validation of a stimulus basis for measuring perception of whole body expression of emotions. Frontiers in Psychology 2:181. doi:10.3389/fpsyg.2011.0018.</td><td>http://www.beatricedegelder.com/beast.html</td><td>None</td><td>Movement</td><td>Whole body expressions</td><td></td><td></td>
<tr><th>Speech Accent Archive</th><td>Weinberger, Steven. (2015). Speech Accent Archive. George Mason University. Retrieved from http://accent.gmu.edu</td><td>http://accent.gmu.edu/howto.php</td><td>CC BY-NC-SA 2.0</td><td>Audio</td><td>accents</td><td></td><td></td>
<tr><th>Massive Auditory Lexical Decision Pseudowords</th><td>Tucker, B. V., Brenner, D., Danielson, D. K., Kelley, M. C., Nenadić, F., & Sims, M. (2019). The massive auditory lexical decision (MALD) database. Behavior research methods, 51, 1187-1204.</td><td>https://doi.org/10.7939/r3-v7jh-p314 </td><td>CC BY-NC 4.0</td><td>Audio</td><td>data set for speech and psycholinguistic research, 9592 pseudowords</td><td></td><td></td>
<tr><th>Massive Auditory Lexical Decision Words</th><td>Tucker, B. V., Brenner, D., Danielson, D. K., Kelley, M. C., Nenadić, F., & Sims, M. (2019). The massive auditory lexical decision (MALD) database. Behavior research methods, 51, 1187-1204.</td><td>https://doi.org/10.7939/r3-v0jr-rr12</td><td>CC BY-NC 4.0</td><td>Audio</td><td>data set for speech and psycholinguistic research, 26,793 words</td><td></td><td></td>


        
        
                <hr>
       </section>
      <footer>
            <p>This project is maintained by <a href="https://github.com/HopeRDA">Dr Glen Pennington</a></p>
            <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="{{ "/assets/js/scale.fix.js" | relative_url }}"></script>
  </body>
</html>  
